\section{Research Questions and Hypotheses}

\subsection{Research Questions (RQs)}
To address the identified gaps, this thesis poses the following research questions:

\begin{enumerate}
    \item \textbf{RQ1:} How does the performance of a Neural Network trained via Federated Learning compare to models trained on isolated local data in terms of fraud detection accuracy?
    \item \textbf{RQ2:} Can the Federated Averaging (FedAvg) algorithm effectively converge when participating banks have highly non-IID fraud distributions?
    \item \textbf{RQ3:} What is the impact of participating in FL on the False Positive Rate (FPR) for individual banks?
    \item \textbf{RQ4:} Is a Neural Network architecture sufficient to replace traditional tree-based models for tabular fraud data in a federated setting?
    \item \textbf{RQ5:} How does the number of participating banks affect the convergence speed of the global model?
    \item \textbf{RQ6:} Can a standardized feature engineering schema effectively bridge the gap between heterogeneous raw banking databases?
    \item \textbf{RQ7:} What is the communication overhead required for synchronizing model parameters in a realistic banking network?
    \item \textbf{RQ8:} Does FL improve detection of "new" fraud patterns that a specific bank has not yet encountered?
    \item \textbf{RQ9:} How resilient is the proposed FL system to a participating bank dropping out during training?
    \item \textbf{RQ10:} What are the minimum local training epochs key to balancing communication cost and model accuracy?
\end{enumerate}

\subsection{Hypotheses}
From these questions, we derive the following testable hypotheses:

\begin{enumerate}
    \item \textbf{H1:} The global FL model will achieve a higher Area Under the Precision-Recall Curve (AUPRC) than the average AUPRC of models trained only on local data.
    \item \textbf{H2:} Participating in FL will reduce the False Negative Rate for "rare" fraud types that are present in the collective dataset but sparse locally.
    \item \textbf{H3:} The FedAvg algorithm will converge to a stable global model even when local fraud rates vary by an order of magnitude (non-IID).
    \item \textbf{H4:} A Neural Network with appropriate embedding layers for categorical variables will perform comparably to a centralized Random Forest baseline.
    \item \textbf{H5:} Increasing the number of local epochs (E) reduces the total communication rounds required for convergence.
    \item \textbf{H6:} The standardized feature vector approach allows for effective model aggregation without requiring raw data alignment.
\end{enumerate}

\section{Theory and Model Selection}

\subsection{Choice of Model: Neural Networks}
While tree-based models like Random Forest and XGBoost are industry standards for tabular data, they pose significant challenges in a Federated Learning context. Aggregating decision trees from different clients is non-trivial because trees partition the feature space differently based on local data distributions. Averaging tree structures or leaf values is mathematically complex and often leads to performance degradation or privacy leakage (via threshold exposure).

In contrast, \textbf{Neural Networks (NN)} are chosen for this research because:
\begin{itemize}
    \item \textbf{Gradient-Based Optimization:} NNs are optimized using stochastic gradient descent (SGD), which naturally fits the FedAvg algorithm \citep{mcmahan2017communication}.
    \item \textbf{Parameter Aggregation:} The weights of a NN are real-valued matrices that can be averaged element-wise. This provides a clear mathematical foundation for the aggregation step: $w_{global} = \sum \frac{n_k}{n} w_k$.
    \item \textbf{Non-Linearity:} NNs can learn complex, non-linear patterns in fraud data through hidden layers and activation functions (ReLU), approximating the decision boundaries that decision trees would find.
\end{itemize}

\subsection{Federated Averaging (FedAvg)}
The core algorithm used is FedAvg. The optimization objective is:
\begin{equation}
    \min_{w} F(w) = \sum_{k=1}^{K} \frac{n_k}{n} F_k(w)
\end{equation}
where $F_k(w)$ is the local loss function at bank $k$.
In each round $t$, the server distributes the current global model $w_t$. Each bank $k$ performs $E$ epochs of local training: $w_{t+1}^k \leftarrow w_t - \eta \nabla F_k(w_t)$. The server then aggregates: $w_{t+1} \leftarrow \sum_{k=1}^K \frac{n_k}{n} w_{t+1}^k$.

\section{Data and Feature Engineering}

\subsection{Data Sources}
The system relies on internal transaction databases at each bank. Labels ($y \in \{0, 1\}$) are derived from:
\begin{itemize}
    \item Customer disputes and chargebacks.
    \item Investigations by fraud analyst teams.
    \item Confirmed rule-based triggers.
\end{itemize}
Crucially, labels are accurate ground truths derived from historical outcomes, not generated by the model itself.

\subsection{Feature Standardization Rule}
A critical constraint of this FL framework is that while \textit{raw data} storage formats may differ across banks (e.g., 'Oracle' vs 'PostgreSQL', different column names), the \textbf{Feature Vector} input to the model must be identical in structure.

\textbf{Defined Schema:}
\begin{table}[H]
    \centering
    \begin{tabular}{p{4.0cm} p{10.0cm}}
        \toprule
        \textbf{Feature} & \textbf{Description} \\
        \midrule
        \texttt{txn\_amount} & Normalized transaction value \\
        \texttt{txn\_hour} & Hour of day (0-23) \\
        \texttt{merchant\_cat} & Category code (One-hot encoded) \\
        \texttt{is\_foreign} & Binary flag for international txn \\
        \texttt{device\_hash} & High-cardinality categorical (Embedding) \\
        \bottomrule
    \end{tabular}
    \caption{Standardized Feature Schema for FL}
\end{table}

\subsection{Handling Missing Data}
If a bank does not collect a specific feature (e.g., \texttt{device\_hash}), the protocol mandates filling this with a sentinel value (e.g., 0 or 'UNKNOWN') rather than omitting the column, ensuring the vector dimensions remain $d \times 1$ for all clients.

\section{System Workflow}

The operational workflow for the proposed Federated Learning system is as follows:

\begin{enumerate}
    \item \textbf{Initialization:} The central server utilizes a random seed or a pre-trained base to initialize the weights $w_0$ of the Neural Network.
    \item \textbf{Distribution:} The server broadcasts $w_0$ to all participating banks $K$.
    \item \textbf{Local Training (Client-Side):}
    \begin{itemize}
        \item Bank $k$ loads $w_0$ into its local infrastructure.
        \item It trains the model on its private dataset $D_k$ for $E$ epochs using a local optimizer (e.g., Adam).
        \item Result: Local weights $w_t^k$.
    \end{itemize}
    \item \textbf{Upload:} Bank $k$ uploads the weight update $\Delta w = w_t^k$ (or the weights themselves) to the server. This step is secured via TLS and potentially Secure Aggregation.
    \item \textbf{Aggregation (Server-Side):} The server computes the weighted average of the received models: $w_{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_t^k$.
    \item \textbf{Iteration:} The new global model $w_{t+1}$ is broadcast back to banks. Steps 3-5 repeat until the loss function converges or a fixed number of rounds is reached.
    \item \textbf{Deployment:} The final converged model is frozen and deployed into the production transaction processing pipeline of each bank for real-time inference.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/system_architecture.png}
    \caption{High-Level System Architecture: Hub-and-Spoke Topology. The Central Aggregator coordinates training without access to private banking data.}
    \label{fig:architecture}
\end{figure}

\section{Data Harmonization \& Feature Engineering}

\subsection{Data Pipeline Overview}
Given the heterogeneity of raw data across banks, a robust data pipeline is essential. This pipeline ensures that despite differences in raw data formats or missing fields, the input to the Neural Network remains consistent and standardized.

\begin{itemize}
    \item \textbf{Raw Data Collection:} Ingesting transaction data from various sources within the bank.
    \item \textbf{Schema Mapping:} Aligning incoming data to the predefined Common Schema.
    \item \textbf{Feature Engineering:} Deriving additional features or transforming existing ones to better represent the underlying patterns.
    \item \textbf{Normalization:} Scaling features to a standard range, ensuring no single feature dominates due to its scale.
    \item \textbf{Encoding:} Converting categorical variables into a numerical format that can be fed into the Neural Network.
\end{itemize}

\subsection{Feature Engineering Details}
To implement the Common Schema, each bank performs specific transformations on its raw data:

\begin{table}[H]
    \centering
    \begin{tabular}{p{3.0cm} p{2.0cm} p{8.5cm}}
        \toprule
        \textbf{Feature} & \textbf{Type} & \textbf{Transformation} \\
        \midrule
        \texttt{txn\_amount} & Float & \textbf{Min-Max Scaler}: Scales the transaction amount to [0, 1]. \\
        \texttt{txn\_hour} & Int & \textbf{One-Hot Encoder}: 24 binary features for each hour. \\
        \texttt{merchant\_cat} & Categorical & \textbf{Target Encoder}: Replaces category with mean target value. \\
        \texttt{is\_foreign} & Binary & \textbf{Leave As Is}: Already in a suitable format. \\
        \texttt{device\_hash} & Categorical & \textbf{Frequency Encoder}: Replaces hash with its frequency. \\
        \texttt{dist\_dev} & Float & \textbf{Standard Scaler}: Z-score normalization of deviation from user's avg transaction amount. \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/data_pipeline.png}
    \caption{Data Harmonization Pipeline: Transforming heterogeneous raw data into the Standardized Feature Vector (X).}
    \label{fig:pipeline}
\end{figure}

\subsection{Handling Schema Mismatch}
In cases where a bank's raw data schema lacks certain features present in the Common Schema:

\begin{itemize}
    \item The missing feature is filled with a sentinel value (e.g., 0, 'UNKNOWN').
    \item This ensures all feature vectors have the same dimensionality, crucial for model training.
\end{itemize}

\section{System Workflow}

The operational workflow for the proposed Federated Learning system is as follows:

\begin{enumerate}
    \item \textbf{Initialization:} The central server utilizes a random seed or a pre-trained base to initialize the weights $w_0$ of the Neural Network.
    \item \textbf{Distribution:} The server broadcasts $w_0$ to all participating banks $K$.
    \item \textbf{Local Training (Client-Side):}
    \begin{itemize}
        \item Bank $k$ loads $w_0$ into its local infrastructure.
        \item It trains the model on its private dataset $D_k$ for $E$ epochs using a local optimizer (e.g., Adam).
        \item Result: Local weights $w_t^k$.
    \end{itemize}
    \item \textbf{Upload:} Bank $k$ uploads the weight update $\Delta w = w_t^k$ (or the weights themselves) to the server. This step is secured via TLS and potentially Secure Aggregation.
    \item \textbf{Aggregation (Server-Side):} The server computes the weighted average of the received models: $w_{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_t^k$.
    \item \textbf{Iteration:} The new global model $w_{t+1}$ is broadcast back to banks. Steps 3-5 repeat until the loss function converges or a fixed number of rounds is reached.
    \item \textbf{Deployment:} The final converged model is frozen and deployed into the production transaction processing pipeline of each bank for real-time inference.
\end{enumerate}

\subsection{Phase 1: Initialization}
The Central Aggregator initializes a global Neural Network (Multi-Layer Perceptron) with random weights $w_0$. The architecture is hardcoded and shared with all clients:
\begin{equation*}
\text{Input} \to \text{Dense}_{128} \to \text{ReLU} \to \text{Dropout}_{0.2} \to \text{Dense}_{64} \to \text{ReLU} \to \text{Dense}_{1} \to \text{Sigmoid}
\end{equation*}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/nn_architecture.png}
    \caption{Neural Network Architecture designed for Federated Fraud Detection.}
    \label{fig:nn_arch}
\end{figure}

\subsection{Phase 2: Distribution \& Local Training}
Once initialized, the global model $w_0$ is sent to all participating banks. Each bank then:

\begin{itemize}
    \item Receives the global model weights.
    \item Performs local computations:
    \begin{itemize}
        \item Performing local Feature Engineering to map raw data to the Common Schema.
        \item Executing Stochastic Gradient Descent (SGD) on local data to derive model updates ($\Delta w_k$).
        \item Communicating only the model parameters (weights) back to the server.
    \end{itemize}
\end{itemize}

\subsection{Phase 3: Aggregation \& Termination}
The server collects updates. Once the threshold of responses is met, it applies the aggregation formula. This process repeats until the global validation loss stabilizes or maximum rounds $T$ is reached. The final model $w_{final}$ is then deployed to all banks for local inference.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sequence_diagram.png}
    \caption{Sequence Diagram of a single Federated Learning training round.}
    \label{fig:sequence}
\end{figure}
